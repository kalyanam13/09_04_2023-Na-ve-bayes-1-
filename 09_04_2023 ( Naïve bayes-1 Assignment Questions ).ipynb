{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6ef2e5",
   "metadata": {},
   "source": [
    "# PW SKILLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dce8336",
   "metadata": {},
   "source": [
    "## Assignment Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09bc15",
   "metadata": {},
   "source": [
    "### Q1. What is Bayes' theorem?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe1ea5",
   "metadata": {},
   "source": [
    "Bayes' Theorem is a fundamental concept in probability theory that describes the probability of an event based on prior knowledge of conditions related to the event. Named after the Reverend Thomas Bayes, who introduced the theorem, it is a fundamental principle in Bayesian statistics and has applications in various fields, including machine learning, artificial intelligence, and decision theory.\n",
    "\n",
    "The general form of Bayes' Theorem is expressed as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)⋅P(A)\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(A∣B) is the probability of event A occurring given that event B has occurred (the posterior probability).\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(B∣A) is the probability of event B occurring given that event A has occurred (the likelihood).\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(A) is the probability of event A occurring (the prior probability).\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(B) is the probability of event B occurring.\n",
    "In words, Bayes' Theorem states that the probability of A given B is proportional to the likelihood of B given A, multiplied by the prior probability of A, and divided by the overall probability of B.\n",
    "\n",
    "This theorem is particularly powerful in updating beliefs or probabilities based on new evidence. It is commonly used in Bayesian inference, where prior knowledge is combined with observed data to make inferences about unknown quantities.\n",
    "\n",
    "In the context of machine learning, Bayes' Theorem is a key component of Naive Bayes classifiers, a class of probabilistic classifiers that make predictions based on the application of Bayes' Theorem.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2bd32d",
   "metadata": {},
   "source": [
    "### Q2. What is the formula for Bayes' theorem?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ccf039",
   "metadata": {},
   "source": [
    "Bayes' Theorem is expressed by the following formula:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)⋅P(A)\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(A∣B) is the probability of event A occurring given that event B has occurred (the posterior probability).\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(B∣A) is the probability of event B occurring given that event A has occurred (the likelihood).\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(A) is the probability of event A occurring (the prior probability).\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(B) is the probability of event B occurring.\n",
    "This formula provides a way to update our beliefs about the probability of event A occurring, given new evidence or information (event B). It is a fundamental concept in Bayesian statistics and is widely used in various fields, including machine learning and artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ca7a4",
   "metadata": {},
   "source": [
    "### Q3. How is Bayes' theorem used in practice?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9be0ed",
   "metadata": {},
   "source": [
    "Bayes' Theorem is used in practice in various fields and applications, and its practical utility lies in updating probabilities based on new evidence or information. Here are some common ways in which Bayes' Theorem is applied:\n",
    "\n",
    "Bayesian Inference:\n",
    "\n",
    "Application: Bayes' Theorem is a cornerstone in Bayesian inference, where it is used to update prior beliefs about the probability of a hypothesis given new evidence.\n",
    "Example: In medical diagnosis, Bayes' Theorem can be used to update the probability of a disease given the results of a diagnostic test.\n",
    "Naive Bayes Classifiers:\n",
    "\n",
    "Application: In machine learning, particularly in text classification, Naive Bayes classifiers use Bayes' Theorem to make predictions based on features.\n",
    "Example: Email spam filtering, where the algorithm uses the probability of certain words occurring in spam or non-spam emails to classify new emails.\n",
    "Medical Decision Making:\n",
    "\n",
    "Application: Bayes' Theorem is employed in medical decision-making processes, especially in diagnostic tests and treatment planning.\n",
    "Example: Estimating the probability of a patient having a particular disease based on symptoms and test results.\n",
    "Finance and Risk Management:\n",
    "\n",
    "Application: Bayes' Theorem is used in finance for risk assessment, particularly in Bayesian portfolio optimization and updating probability distributions.\n",
    "Example: Evaluating the probability of a financial event, such as a market crash, based on historical data and market conditions.\n",
    "Quality Control and Manufacturing:\n",
    "\n",
    "Application: In quality control processes, Bayes' Theorem can be used to update the probability of a defect given new information from inspections.\n",
    "Example: Assessing the probability of a manufactured product being defective based on quality control tests.\n",
    "Natural Language Processing:\n",
    "\n",
    "Application: Bayes' Theorem is applied in various natural language processing tasks, such as sentiment analysis and language modeling.\n",
    "Example: Predicting the sentiment of a sentence or document based on the probability of certain words or phrases occurring.\n",
    "A/B Testing in Marketing:\n",
    "\n",
    "Application: Bayes' Theorem is utilized in A/B testing to analyze and update the probabilities of different versions of a marketing campaign leading to desired outcomes.\n",
    "Example: Evaluating the probability of a new website design leading to more conversions compared to the old design.\n",
    "In each of these applications, Bayes' Theorem enables a systematic and principled way to incorporate new evidence or information into existing beliefs, leading to more accurate and updated probability estimates. It serves as a foundation for probabilistic reasoning and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0c1c6",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef43a173",
   "metadata": {},
   "source": [
    "Bayes' Theorem is closely related to conditional probability, and it provides a way to update prior probabilities based on new evidence or conditions. Let's explore the relationship between Bayes' Theorem and conditional probability:\n",
    "\n",
    "Bayes' Theorem:\n",
    "\n",
    "Formula: Bayes' Theorem is expressed as:\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)⋅P(A)\n",
    "​\n",
    " \n",
    "Interpretation: It calculates the probability of event A occurring given that event B has occurred (posterior probability) by combining the likelihood of B given A, the prior probability of A, and the overall probability of B.\n",
    "Conditional Probability:\n",
    "\n",
    "Definition: Conditional probability, denoted as \n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(A∣B), is the probability of event A occurring given that event B has occurred.\n",
    "Formula: \n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "∩\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(A∩B)\n",
    "​\n",
    " \n",
    "Interpretation: It measures the probability of A occurring within the subset of outcomes where B has already occurred.\n",
    "Connection:\n",
    "\n",
    "Similarity: If we compare the formula for conditional probability with Bayes' Theorem, we can see a similarity:\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "∩\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(A∩B)\n",
    "​\n",
    " \n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)⋅P(A)\n",
    "​\n",
    " \n",
    "Link: Bayes' Theorem incorporates the idea of conditional probability by expressing the probability of A given B in terms of the likelihood of B given A and the prior probability of A.\n",
    "Updating Probabilities:\n",
    "\n",
    "Bayesian Perspective: Bayes' Theorem is particularly powerful for updating probabilities in a Bayesian framework. It starts with prior beliefs (prior probability) and updates them based on new evidence (likelihood), resulting in updated beliefs (posterior probability).\n",
    "Sequential Update:\n",
    "\n",
    "Iteration: Bayes' Theorem can be iteratively applied as new evidence becomes available, allowing a sequential update of probabilities based on multiple pieces of information.\n",
    "In summary, Bayes' Theorem and conditional probability are intricately connected. Bayes' Theorem extends the concept of conditional probability by providing a systematic way to update probabilities using prior knowledge and new evidence. It is a fundamental tool in Bayesian statistics and has broad applications in various fields where updating beliefs based on new information is crucial.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea2d65",
   "metadata": {},
   "source": [
    "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88e87a",
   "metadata": {},
   "source": [
    "Choosing the appropriate type of Naive Bayes classifier for a given problem depends on the characteristics of the data and the assumptions that align with the underlying distribution of the features. There are three main types of Naive Bayes classifiers:\n",
    "\n",
    "Gaussian Naive Bayes:\n",
    "\n",
    "Assumption: Assumes that the features follow a Gaussian (normal) distribution.\n",
    "Use Case: Suitable when continuous or numerical features are present and the assumption of normal distribution holds reasonably well.\n",
    "Example: Classifying email as spam or non-spam based on features like the frequency of certain words, where word frequencies may be normally distributed.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Assumption: Assumes that features are categorical and follow a multinomial distribution (e.g., word counts in document classification).\n",
    "Use Case: Commonly used for text classification problems where the data is represented as word frequency vectors.\n",
    "Example: Classifying documents into topics based on the frequency of words occurring in the document.\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Assumption: Assumes that features are binary (Bernoulli distributed) or follow a categorical distribution.\n",
    "Use Case: Suitable for binary feature data, often used in document classification or sentiment analysis.\n",
    "Example: Classifying documents as positive or negative based on the presence or absence of certain keywords.\n",
    "Guidelines for Choosing:\n",
    "\n",
    "Feature Distribution:\n",
    "\n",
    "Continuous Features: If your features are continuous and approximately follow a normal distribution, Gaussian Naive Bayes might be suitable.\n",
    "Categorical Features: If your features are counts or frequencies (non-negative integers) and represent discrete categories, consider Multinomial Naive Bayes.\n",
    "Binary Features: If your features are binary (0 or 1), representing the presence or absence of a feature, Bernoulli Naive Bayes could be appropriate.\n",
    "Nature of Data:\n",
    "\n",
    "Text Data: For text classification problems, Multinomial or Bernoulli Naive Bayes is often used, depending on how the text data is represented (word frequencies or binary presence/absence).\n",
    "Continuous Data: For problems with continuous data and a Gaussian distribution assumption, Gaussian Naive Bayes may be appropriate.\n",
    "Model Assumptions:\n",
    "\n",
    "Assess Assumptions: Consider the assumptions of each Naive Bayes classifier and assess whether they align with the characteristics of your data.\n",
    "Flexibility: If the assumptions of one type do not hold, explore the others. Naive Bayes classifiers are known for their simplicity and can be surprisingly effective even when assumptions are not perfectly met.\n",
    "Performance Evaluation:\n",
    "\n",
    "Cross-Validation: Evaluate the performance of different Naive Bayes classifiers using cross-validation or other appropriate methods.\n",
    "Choose Based on Performance: Select the classifier that provides the best performance on your specific problem.\n",
    "In practice, it's often beneficial to try multiple Naive Bayes classifiers and compare their performance on your specific dataset. Additionally, considering the interpretability and ease of implementation of each type can also influence the choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52c43a",
   "metadata": {},
   "source": [
    "### Q6. Assignment:\n",
    "### You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    "Class\t X1=1 X1=2 \tX1=3 \tX2=1 \tX2=2 \tX2=3\t X2=4\n",
    "\n",
    "     A\t 3\t 3\t 4\t 4\t 3\t 3\t 3\n",
    "\n",
    "     B\t 2\t 2\t 1\t 2\t 2\t 2\t 3\n",
    "     \n",
    "### Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e816e7",
   "metadata": {},
   "source": [
    "To classify a new instance with features \n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    "X \n",
    "1\n",
    "​\n",
    " =3 and \n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    "X \n",
    "2\n",
    "​\n",
    " =4 using Naive Bayes, we need to calculate the posterior probabilities for each class and then choose the class with the highest posterior probability. The Naive Bayes assumption implies that the features are conditionally independent given the class.\n",
    "\n",
    "Let \n",
    "�\n",
    "A be the event that the class is A, and \n",
    "�\n",
    "B be the event that the class is B.\n",
    "\n",
    "The posterior probability for class A is given by:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "∝\n",
    "�\n",
    "(\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    "∣\n",
    "�\n",
    ")\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    "∣\n",
    "�\n",
    ")\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(A∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)∝P(X \n",
    "1\n",
    "​\n",
    " =3∣A)⋅P(X \n",
    "2\n",
    "​\n",
    " =4∣A)⋅P(A)\n",
    "\n",
    "Similarly, the posterior probability for class B is given by:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "∝\n",
    "�\n",
    "(\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    "∣\n",
    "�\n",
    ")\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    "∣\n",
    "�\n",
    ")\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(B∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)∝P(X \n",
    "1\n",
    "​\n",
    " =3∣B)⋅P(X \n",
    "2\n",
    "​\n",
    " =4∣B)⋅P(B)\n",
    "\n",
    "Given that the prior probabilities are equal (\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(A)=P(B)), we can ignore the denominator in the proportional expression.\n",
    "\n",
    "Now, let's calculate the probabilities:\n",
    "\n",
    "For Class A:\n",
    "�\n",
    "(\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "4\n",
    "10\n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3∣A)= \n",
    "10\n",
    "4\n",
    "​\n",
    " \n",
    "�\n",
    "(\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "3\n",
    "10\n",
    "P(X \n",
    "2\n",
    "​\n",
    " =4∣A)= \n",
    "10\n",
    "3\n",
    "​\n",
    " \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "2\n",
    "P(A)= \n",
    "2\n",
    "1\n",
    "​\n",
    " \n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "∝\n",
    "4\n",
    "10\n",
    "⋅\n",
    "3\n",
    "10\n",
    "⋅\n",
    "1\n",
    "2\n",
    "P(A∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)∝ \n",
    "10\n",
    "4\n",
    "​\n",
    " ⋅ \n",
    "10\n",
    "3\n",
    "​\n",
    " ⋅ \n",
    "2\n",
    "1\n",
    "​\n",
    " \n",
    "\n",
    "For Class B:\n",
    "�\n",
    "(\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "7\n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3∣B)= \n",
    "7\n",
    "1\n",
    "​\n",
    " \n",
    "�\n",
    "(\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "7\n",
    "P(X \n",
    "2\n",
    "​\n",
    " =4∣B)= \n",
    "7\n",
    "1\n",
    "​\n",
    " \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "2\n",
    "P(B)= \n",
    "2\n",
    "1\n",
    "​\n",
    " \n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "∝\n",
    "1\n",
    "7\n",
    "⋅\n",
    "1\n",
    "7\n",
    "⋅\n",
    "1\n",
    "2\n",
    "P(B∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)∝ \n",
    "7\n",
    "1\n",
    "​\n",
    " ⋅ \n",
    "7\n",
    "1\n",
    "​\n",
    " ⋅ \n",
    "2\n",
    "1\n",
    "​\n",
    " \n",
    "\n",
    "Now, we compare the posterior probabilities:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "∝\n",
    "4\n",
    "10\n",
    "⋅\n",
    "3\n",
    "10\n",
    "⋅\n",
    "1\n",
    "2\n",
    "≈\n",
    "0.06\n",
    "P(A∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)∝ \n",
    "10\n",
    "4\n",
    "​\n",
    " ⋅ \n",
    "10\n",
    "3\n",
    "​\n",
    " ⋅ \n",
    "2\n",
    "1\n",
    "​\n",
    " ≈0.06\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "∝\n",
    "1\n",
    "7\n",
    "⋅\n",
    "1\n",
    "7\n",
    "⋅\n",
    "1\n",
    "2\n",
    "≈\n",
    "0.01\n",
    "P(B∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)∝ \n",
    "7\n",
    "1\n",
    "​\n",
    " ⋅ \n",
    "7\n",
    "1\n",
    "​\n",
    " ⋅ \n",
    "2\n",
    "1\n",
    "​\n",
    " ≈0.01\n",
    "\n",
    "The Naive Bayes classifier would predict the new instance to belong to Class A because it has a higher posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1807a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e2a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
